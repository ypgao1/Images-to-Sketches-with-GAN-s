{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from math import log10\n",
    "import json\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from statistics import mean\n",
    "  \n",
    "\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import time\n",
    "from generators.generators import create_gen\n",
    "from discriminators.discriminators import create_disc\n",
    "from datasets.datasets import get_dataset\n",
    "from util import ImagePool, set_requires_grad,tensor_to_plt,init_weights, mkdir\n",
    "from Tensorboard_Logger import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer ):\n",
    "    '''\n",
    "    Learning rate scheduler. We want to start off at a constant rate and slowly decay\n",
    "    '''\n",
    "    def lambda_rule(epoch):\n",
    "        lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.iter_constant) / float(opt.iter_decay + 1)\n",
    "        return lr_l\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "class Train_Pix2Pix:\n",
    "    '''\n",
    "    GAN model for Pix2Pix implementation. Trains models, saves models, saves training results and details about the models.\n",
    "    '''\n",
    "    def __init__(self,opt,traindataset,testdataset):\n",
    "        \n",
    "        #load in the datasets\n",
    "        self.dataset = DataLoader(dataset=traindataset, batch_size=opt.batch_size, shuffle=True,num_workers=opt.threads)\n",
    "        self.test_set = DataLoader(dataset=testdataset, batch_size=opt.test_batch_size, shuffle=False,num_workers=opt.threads)\n",
    "        self.atest, self.btest = next(iter(self.test_set))\n",
    "        self.dataviz = DataLoader(dataset=traindataset, batch_size=opt.batch_size, shuffle=False,num_workers=opt.threads)\n",
    "        self.atrain, self.btrain = next(iter(self.dataviz))\n",
    "\n",
    "        #tensorflow logger\n",
    "        self.device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "        self.writer = Logger(opt.folder_name)\n",
    "        self.writer.write_photo_to_tb(self.atest,\"photos test\")\n",
    "        self.writer.write_sketch_to_tb(self.btest,\"sketches test\")\n",
    "        self.writer.write_photo_to_tb(self.atrain,\"photos train\")\n",
    "        self.writer.write_sketch_to_tb(self.btrain,\"sketches train\")\n",
    "\n",
    "\n",
    "        #create generator and discriminator\n",
    "        self.netG = create_gen(opt.gen,opt.input_dim,opt.output_dim,opt.gen_filters,opt.norm)\n",
    "        self.netG.to(self.device)\n",
    "        init_weights(self.netG)\n",
    "        use_sigmoid = True if opt.loss == \"bce\" else False\n",
    "        self.netD = create_disc(opt.disc,opt.input_dim+opt.output_dim,use_sigmoid)\n",
    "        self.netD.to(self.device)\n",
    "        init_weights(self.netD)\n",
    "\n",
    "        #set the GAN adversarial loss\n",
    "        if opt.loss ==\"bce\":\n",
    "            self.criterion = nn.BCELoss().to(self.device) \n",
    "        elif opt.loss == \"ls\":\n",
    "            self.criterion = nn.MSELoss().to(self.device)\n",
    "        elif opt.loss == \"wloss\":\n",
    "            self.criterion = self.get_w_loss\n",
    "            \n",
    "        self.real_label_value = 1.0\n",
    "        self.fake_label_value = 0.0\n",
    "        \n",
    "        #optimizers and attach them to schedulers that change learning rate\n",
    "        self.schedulers = []\n",
    "        self.optimizers = []\n",
    "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        self.optimizer_D = torch.optim.Adam(self.netD.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        self.optimizers.append(self.optimizer_G)\n",
    "        self.optimizers.append(self.optimizer_D)\n",
    "        for optimizer in self.optimizers:\n",
    "            self.schedulers.append(get_scheduler(optimizer))\n",
    "\n",
    "        #logging each epoch losses\n",
    "        self.gen_loss = []\n",
    "        self.disc_loss = []\n",
    "        self.l1_loss = []\n",
    "\n",
    "    def train(self,opt):\n",
    "        '''\n",
    "        Starts the training process. The details and parts of the model were already initialized in __init__\n",
    "        '''\n",
    "        for epoch in range(opt.epoch_count, opt.total_iters + 1):\n",
    "\n",
    "            #monitor each minibatch loss\n",
    "            lossdlist = []\n",
    "            lossglist = []\n",
    "            lossl1list = []\n",
    "            \n",
    "            t1 = time.time()\n",
    "            \n",
    "            for i, batch in enumerate(self.dataset):\n",
    "                if i % 10 == 0:\n",
    "                    print(\"training epoch \",epoch,\"batch\", i,\"/\",len(self.dataset))\n",
    "                real_A, real_B = batch[0].to(self.device), batch[1].to(self.device) #load in a batch of data\n",
    "\n",
    "                # (generate fake images)\n",
    "                fake_B = self.netG(real_A)\n",
    "\n",
    "                #Optimize D ################################\n",
    "                set_requires_grad(nets=self.netD, requires_grad=True) #optimize for D so create computation graph\n",
    "                self.optimizer_D.zero_grad()\n",
    "                \n",
    "                fake_AB =torch.cat((real_A, fake_B), 1).detach() #we detach because when we train discriminator we don't want to back propogate the generator\n",
    "                pred_fake = self.netD(fake_AB.detach()) #generate predictions on fake images\n",
    "                \n",
    "                #create labels, either 1's or 0.9 if we're smooting.  \n",
    "                if opt.label_smoothing:\n",
    "                    real_labels = torch.normal(.9, .02, size=pred_fake.size()).to(self.device)\n",
    "                else:\n",
    "                    real_labels = torch.full(pred_fake.size(),self.real_label_value).to(self.device)\n",
    "                fake_labels = torch.full(pred_fake.size(),self.fake_label_value).to(self.device)\n",
    "                \n",
    "                \n",
    "                if not(opt.loss == \"wloss\"):\n",
    "                    #Get the first half of the loss. Comparing fake predictions to labels of 0\n",
    "                    loss_D_fake = self.criterion(pred_fake, fake_labels)\n",
    "\n",
    "                #Get the second half of the loss. Compare real predictions to labels of 1\n",
    "                #Here we have 5 real sketches per image, so we loop it 5 times.\n",
    "                n = real_B.shape[1]\n",
    "                loss_D_real_set = torch.empty(n, device=self.device)\n",
    "                for i in range(n):\n",
    "                    sel_B = real_B[:, i, :, :].unsqueeze(1)\n",
    "                    real_AB = torch.cat((real_A, sel_B), 1)\n",
    "                    pred_real = self.netD(real_AB)\n",
    "                    if not(opt.loss == \"wloss\"): \n",
    "                        loss_D_real_set[i] = self.criterion(pred_real, real_labels)\n",
    "                    else: #if we're using Wassersetin loss\n",
    "                        loss_D_real_set[i] = self.criterion(real_AB,fake_AB, pred_fake, pred_real, opt.lambda_GP)        \n",
    "                loss_D_real = torch.mean(loss_D_real_set)\n",
    "                \n",
    "                if not(opt.loss == \"wloss\"): \n",
    "                    loss_D = (loss_D_fake + loss_D_real) * 0.5 * opt.lambda_G\n",
    "                else:\n",
    "                    loss_D = loss_D_real\n",
    "                \n",
    "                lossdlist.append(loss_D.item())\n",
    "                \n",
    "                #now that we have the full loss we back propogate\n",
    "                loss_D.backward(retain_graph=True if opt.loss == \"wloss\" else False)\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "                # Optimize G #####################################\n",
    "                set_requires_grad(nets=self.netD, requires_grad=False) #dont want the discriminator to update weights this round\n",
    "                self.optimizer_G.zero_grad()\n",
    "\n",
    "                fake_AB = torch.cat((real_A, fake_B), 1)\n",
    "                pred_fake = self.netD(fake_AB) #generate D predictions of fake images\n",
    "                if not(opt.loss == \"wloss\"):\n",
    "                    loss_G_GAN = self.criterion(pred_fake, real_labels) #We feed it real_labels as G is trying fool the discriminator\n",
    "                \n",
    "                loss_G_L1 = self.get_l1_loss(real_B,fake_B) #get per pixel L1 Loss\n",
    "                lossl1list.append( loss_G_L1.item())\n",
    "\n",
    "                if not(opt.loss == \"wloss\"):\n",
    "                    loss_G = loss_G_GAN + loss_G_L1 * opt.lambda_A\n",
    "                else:\n",
    "                    loss_G = pred_fake.mean()*-1 #the Generator Loss in WGAn is different\n",
    "                    \n",
    "                lossglist.append(loss_G.item())\n",
    "                loss_G.backward()\n",
    "                self.optimizer_G.step()\n",
    "\n",
    "            #update_learning_rate()\n",
    "            for scheduler in self.schedulers:\n",
    "                scheduler.step()\n",
    "            lr = self.optimizers[0].param_groups[0]['lr']\n",
    "            print('learning rate = %.7f' % lr)\n",
    "            t2 = time.time()\n",
    "            diff = t2-t1\n",
    "            print(\"iteration:\",epoch,\"loss D:\", mean(lossdlist),\"loss G:\", mean(lossglist))\n",
    "            print(\"Took \", diff, \"seconds\")\n",
    "            print(\"Estimated time left:\", diff*(opt.total_iters - epoch))\n",
    "\n",
    "            self.gen_loss.append(mean(lossglist))\n",
    "            self.disc_loss.append(mean(lossdlist))\n",
    "            self.l1_loss.append(mean(lossl1list))\n",
    "\n",
    "\n",
    "            if epoch % 1 == 0:                \n",
    "                with torch.no_grad():\n",
    "                    out1 = self.netG(self.atrain.to(self.device))\n",
    "                    title= \"Epoch \"+str(epoch) +\"Training\"\n",
    "                    self.writer.write_sketch_to_tb(out1.detach(),title) \n",
    "            \n",
    "                    out2 = self.netG(self.atest.to(self.device))\n",
    "                    title= \"Epoch \"+str(epoch)\n",
    "                    self.writer.write_sketch_to_tb(out2.detach(),title)\n",
    "            \n",
    "\n",
    "            \n",
    "        self.writer.plot_losses(self.gen_loss,self.disc_loss,self.l1_loss)\n",
    "    \n",
    "    def save_model(self,folderpath,modelpath):\n",
    "        '''\n",
    "        Saves the models as well as the optimizers\n",
    "        '''\n",
    "        mkdir(folderpath)\n",
    "        torch.save({\n",
    "            'gen': self.netG.module.state_dict(),\n",
    "            'disc': self.netG.module.state_dict(),\n",
    "            'optimizerG_state_dict': self.optimizer_G.state_dict(),\n",
    "            'optimizerD_state_dict': self.optimizer_D.state_dict(),\n",
    "            \n",
    "            }, modelpath)\n",
    "        \n",
    "    def save_arrays(self,path):\n",
    "        '''\n",
    "        save the losses to numpy arrays\n",
    "        '''\n",
    "        np.save( os.path.join(path,\"ganloss\"),np.asarray(self.gen_loss))\n",
    "        np.save( os.path.join(path,\"discloss\"),np.asarray(self.disc_loss))\n",
    "        np.save( os.path.join(path,\"l1loss\"),np.asarray(self.l1_loss))\n",
    "        \n",
    "    def save_hyper_params(self,folderpath,opt):\n",
    "        '''\n",
    "        We need to load back in the details of the model when we test so we save these as well\n",
    "        '''\n",
    "        with open(os.path.join(folderpath,'params.txt'), 'w') as file:\n",
    "             file.write(json.dumps(opt.__dict__)) \n",
    "        \n",
    "    def get_w_loss(self,real_AB,fake_AB, pred_fake, pred_real, gp_lambda):\n",
    "        '''\n",
    "        Wasserstein Loss with Gradient Penalty\n",
    "        '''\n",
    "        epsilon = torch.rand(len(real_AB), 1, 1, 1, requires_grad=True).to(self.device)\n",
    "        interpolated = real_AB*epsilon + fake_AB * (1 - epsilon)\n",
    "        mixed_pred = self.netD(interpolated)\n",
    "        gradient = torch.autograd.grad(inputs=interpolated,outputs=mixed_pred,\n",
    "                    grad_outputs=torch.ones_like(mixed_pred), create_graph=True,retain_graph=True,)[0]\n",
    "        \n",
    "        gradient = gradient.view(len(gradient), -1)\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        gp = (gradient_norm -1)**2\n",
    "        gp = gp.mean()\n",
    "        \n",
    "        loss = pred_fake.mean() - pred_real.mean() + gp* gp_lambda\n",
    "        return loss\n",
    "    \n",
    "    def get_l1_loss(self,real_B,fake_B):\n",
    "        '''\n",
    "        L1 Loss for Pix2Pix. Compares the Pixel Loss between the fake image and it's closest real image\n",
    "        '''\n",
    "        fake_B_expand = fake_B.expand(-1, 5, -1, -1)\n",
    "        L1 = torch.abs(fake_B_expand - real_B)\n",
    "        L1 = L1.view(-1, 5, real_B.shape[2]*real_B.shape[3])\n",
    "        L1 = torch.mean(L1, 2)\n",
    "        min_L1, min_idx = torch.min(L1, 1)\n",
    "        loss_G_L1 = torch.mean(min_L1)\n",
    "        return loss_G_L1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args():\n",
    "    '''\n",
    "    We set model details as a class that we can pass around\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16 \n",
    "        self.test_batch_size = 16\n",
    "        self.input_dim = 3\n",
    "        self.output_dim = 1\n",
    "        self.gen_filters =64 #starting filters for the generator\n",
    "        self.disc_filters =64 #starting filters for the discriminator\n",
    "        self.epoch_count =1 #starting epoch, useful if we're loading in a half trained model, we can change starting epoch\n",
    "        self.total_iters=100 #total epochs we're training for\n",
    "        self.iter_constant = 200 #how many epochs we keep the learning rate constant\n",
    "        self.iter_decay = 200 #when we start decaying the learning rate\n",
    "        self.lr = 0.0002\n",
    "        self.label_smoothing = True #if True, we use one sided label smoothing\n",
    "        self.beta1 = 0.5 # beta1 for our Adam optimizer\n",
    "        self.cuda = True\n",
    "        self.threads = 8\n",
    "        self.lambda_A = 0 #L1 lambda\n",
    "        self.lambda_GP = 10 #Gradient_penalty loss\n",
    "        self.use_ls = True\n",
    "        self.resblocks = 9 #number of resblocks in bottleneck if we're using resnet generator\n",
    "        self.norm = \"instance\"\n",
    "        self.gen = \"Resnet\" # Resnet, UNet++, UNet, UNet-no-skips\n",
    "        self.disc= \"Global\" #Global, Patch\n",
    "        self.loss = \"wloss\" #ls, bce, wloss\n",
    "        self.paired_dataset = True\n",
    "        self.dataset_name = \"sketchy\" # \"aligned\" or \"sketchy\"\n",
    "        self.flip = True #image augementation flip horizontally \n",
    "        self.jitter = True #image augementation vary color, brightness and contrast\n",
    "        self.erase = True #image augementation randomly erase a portion of input image\n",
    "        self.folder_name = \"wgan_sketchy_resnet\" #where we want to save the model to\n",
    "    \n",
    "    \n",
    "opt = Args()\n",
    "\n",
    "photo_path_train = os.path.join(os.getcwd(),\"data\",opt.dataset_name,\"train\", \"photo\")\n",
    "sketch_path_train = os.path.join(os.getcwd(),\"data\",opt.dataset_name,\"train\", \"sketch\")\n",
    "train_set = get_dataset(photo_path_train,sketch_path_train, opt,flip=True,jitter=True,erase= False)\n",
    "\n",
    "photo_path_test = os.path.join(os.getcwd(),\"data\",opt.dataset_name,\"test\", \"photo\")\n",
    "sketch_path_test = os.path.join(os.getcwd(),\"data\",opt.dataset_name,\"test\", \"sketch\")\n",
    "testing_set =  get_dataset(photo_path_test,sketch_path_test, opt,flip=False,jitter=False,erase= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exps = [opt]\n",
    "for option in exps:\n",
    "    experiment = Train_Pix2Pix(option,train_set,testing_set)\n",
    "    experiment.train(option)\n",
    "    folderpath = os.path.join(os.getcwd(),\"models\",option.folder_name)\n",
    "    model_path = os.path.join(folderpath,option.gen)\n",
    "    experiment.save_model(folderpath,model_path)\n",
    "    experiment.save_arrays(folderpath)\n",
    "    experiment.save_hyper_params(folderpath,opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
